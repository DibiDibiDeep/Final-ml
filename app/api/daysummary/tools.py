import os
from dotenv import load_dotenv
import requests
import logging
from datetime import datetime

from langchain_core.prompts import ChatPromptTemplate
from langchain.agents import tool
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from openai import OpenAI

from .config import collection
from pymilvus import (
    RRFRanker,
)

import json
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.retrievers.self_query.base import SelfQueryRetriever

from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
import os

llm_model = os.getenv("LLM_MODEL")
openai_key = os.getenv("OPENAI_API_KEY")

current_dir = os.path.dirname(os.path.abspath(__file__))
tool_template_dir = os.path.join(current_dir, "prompts/tool")

cls_intent_dir = os.path.join(tool_template_dir, "cls_intent_assistant.txt")
sharing_dir = os.path.join(tool_template_dir, "sharing_assistant.txt")
retriever_dir = os.path.join(tool_template_dir, "retriever_assistant.txt")
write_diary_dir = os.path.join(tool_template_dir, "write_diary_assistant.txt")
except_situation_dir = os.path.join(tool_template_dir, "except_situation_assistant.txt")
# Load the template for the classification intent tool
with open(cls_intent_dir, "r") as f:
    cls_intent_template = f.read()
# Load the template for the empathetic assistant tool
with open(sharing_dir, "r") as f:
    sharing_template = f.read()
# Load the template for the retriever assistant tool
with open(retriever_dir, "r") as f:
    retriever_template = f.read()
# Load the template for the write diary assistant tool
with open(write_diary_dir, "r") as f:
    write_diary_template = f.read()

with open(except_situation_dir, "r") as f:
    except_situation_template = f.read()

# Set up logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

llm_model = os.getenv("LLM_MODEL")
embedding_model = os.getenv("EMBEDDING_MODEL")
embedding_client = OpenAI(api_key=openai_key)


# 사용자 쿼리 의도 분류 도구('QUESTION', 'DIARY_WRITE', 'DIARY_SAVE', 'SHARING', 'EXCEPT_SITUATION')
@tool
def cls_intent_assistant(query: str) -> str:
    """
    This function classifies the intent of a given query.
    This tool using ones per user's original query.
    Parameters:
        query (str): The query input by the user {{"query":str}}
    Returns:
        ```
        str: 'QUESTION', 'DIARY_WRITE', 'DIARY_SAVE', 'SHARING', 'EXCEPT_SITUATION'
        ```
    """
    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        temperature=0,
    )
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                cls_intent_template,
            ),
            ("user", "{query}"),
        ]
    )
    chain = prompt | llm
    response = chain.invoke({"query": query})
    return response.content.strip().upper()


# 예외 상황 처리 도구(프롬프트 인젝선 및 검색 결과 없을 때)
@tool
def except_situation_assistant(query_and_thought: str) -> str:
    """
    This function generates a response when it's difficult to choose the next Action during the Thought process.
    When retriever_assistant tool returns False or false, this tool is used to generate a response.
    Parameters:
        query_and_thought (str): The user's query and the agent's thought. {{"query": query: str, "thought": thought: str}}
    Returns:
        ```
        str: The response generated by the assistant
        ```
    """
    if "{" not in query_and_thought:
        query_and_thought = "{" + query_and_thought + "}"
    logging.info(f"Input string: {query_and_thought}")
    input_json = json.loads(query_and_thought)

    query = input_json["query"]
    thought = input_json["thought"]

    chain = (
        PromptTemplate.from_template(except_situation_template)
        | ChatOpenAI(
            model=llm_model,
            openai_api_key=openai_key,
        )
        | StrOutputParser()
    )

    return chain.invoke({"query": query, "thought": thought})


# 사용자 쿼리 의도가 'SHARING'일 때 사용되는 도구(공감 대화 생성)
@tool
def sharing_assistant(query_and_chat_history: str) -> str:
    """
    When the user query intent is 'SHARING', this tool is used to generate empathetic responses.
    Parameters:
        query_and_chat_history (str): The user's query. The chat history between the user and the assistant. {{"query": query: str, "chat_history": chat_history: str}}
    Returns:
        ```
        Thought: agent thought here
        Final Answer: str: The empathetic response generated by the assistant
        ```
    """
    if "{" not in query_and_chat_history:
        query_and_chat_history = "{" + query_and_chat_history + "}"
    logging.info(f"Input string: {query_and_chat_history}")
    input_json = json.loads(query_and_chat_history)

    query = input_json["query"]
    chat_history = input_json["chat_history"]
    chain = (
        PromptTemplate.from_template(sharing_template)
        | ChatOpenAI(
            model=llm_model,
            openai_api_key=openai_key,
        )
        | StrOutputParser()
    )

    return chain.invoke({"query": query, "chat_history": chat_history})


# 사용자 쿼리 의도가 'DIARY_WRITE'일 때 사용되는 도구(일기 생성)
@tool
def write_diary_assistant(day_information: str) -> str:
    """
    When user query intent is 'DIARY_WRITE', this tool is used to generate diary entries based on user,baby information and chat history.
    Parameters:
        day_information (str): The information about the user and baby for the day {{"day_information": str}}
    Returns:
        ```
        str: The diary entry generated by the assistant
        ```
    """
    chain = (
        PromptTemplate.from_template(write_diary_template)
        # OpenAI의 LLM을 사용합니다.
        | ChatOpenAI(
            model=llm_model,
            openai_api_key=openai_key,
        )
        | StrOutputParser()
    )

    return chain.invoke({"day_information": day_information})


# 사용자 질의 임베딩 생성 함수
def get_embedding(client, text, model=embedding_model):
    text = text.replace("\n", " ")
    return client.embeddings.create(input=[text], model=model).data[0].embedding


# 사용자 쿼리 의도가 'QUESTION'일 때 사용되는 도구(특정 정보 검색)
@tool
def retriever_assistant(input_str: str) -> str:
    """
    Use this tool when you need to find specific information about the parent's or child's events.
    Retrieves information about the parent's or child's day and activities and generates a response.
    Parameters:
        input_str:str {{"user_id":int, "baby_id":int, "query":str}}
    returns:
        ```
        str: Retrieved information or a message indicating no information is available.
        ```
    """
    if "{" not in input_str:
        input_str = "{" + input_str + "}"
    logging.info(f"Input string: {input_str}")
    input_json = json.loads(input_str)

    user_id = input_json["user_id"]
    baby_id = input_json["baby_id"]
    query = input_json["query"]
    today_date = datetime.now().strftime("%Y-%m-%d")

    logging.info(
        f"Input parameters - user_id: {user_id}, baby_id: {baby_id}, query: {query}, today_date: {today_date}"
    )
    llm = ChatOpenAI(
        openai_api_key=openai_key,
        model=llm_model,
        temperature=0.0,
    )
    prompt = PromptTemplate.from_template(retriever_template)

    chain = prompt | llm | StrOutputParser()

    # 쿼리 표현식 생성
    expr = chain.invoke(
        {
            "query": query,
            "today_date": today_date,
            "user_id": user_id,
            "baby_id": baby_id,
        }
    )
    logging.info(f"Generated expression: {expr}")
    # 쿼리 임베딩
    query_embeddings = get_embedding(embedding_client, query)

    res = collection.search(
        [query_embeddings],
        expr=expr,
        anns_field="embedding",
        param={
            "metric_type": "COSINE",
            "params": {"nprobe": 10},
        },  # COSINE 메트릭 타입 사용
        rerank=RRFRanker(),
        limit=1,
        output_fields=["date", "text"],
    )
    if len(res[0]) == 0:
        return "No results found"
    return res


# 사용자 쿼리 의도가 'DIARY_SAVE'일 때 사용되는 도구(일기 저장)
@tool
def save_diary_assistant(input_str: str) -> bool:
    """
    Use this when you want to POST to a Backend API.
    Be careful to always use double quotes for strings in the json string
    The output will be the text response of the POST request boolean value(True, False).
    If the response is successful, the function will return True. Otherwise, it will return False.
    Parameters:
        input_str (str): {{"userId",int, "babyId":int, "content":str}}
    Returns:
        ```
        str: REST API response status code(200-level codes indicate success)
        ```

    """
    backend_url = os.getenv("BACKEND_API_URL")
    headers = {
        "Content-Type": "application/json",
    }
    logging.info(f"Input string: {input_str}")
    if "{" not in input_str:
        input_str = "{" + input_str + "}"
    data = json.loads(input_str)
    data.update({"date": datetime.now().strftime("%Y-%m-%d")})

    logging.info(f"Data to be saved: {data} ,Data type: {type(data)}")

    response = requests.post(backend_url + "/api/today-sum", json=data, headers=headers)
    response.status_code == requests.codes.ok
    logging.info(f"Response from backend: {response.status_code}")
    return response.status_code
